{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "\n",
    "This project will be working with immigration data and using other sources to make analyses and find relationships to immigration traffic to the differnet parts of the US. We will use the sources to transform the data into a star schema.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "I will plan to create tables that relate to the demographics, temperature, and elevation of the state resulting after cleaning, transforming, and parqueting the files. These tables along with the country, state, and visa codes will become the dimension tables. The immigration table created from the I94 immigration data will be the fact table. I used Apache spark to handle large data and easy transoformations.  \n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. U.S. City Demographic Data: comes from OpenSoft containing details of demographics in each U.S. city.\n",
    "2. World Temperature Data: comes from Kaggle containing temperature history since 1850. \n",
    "3. Airport Code Table: comes from datahub.io containing details of airports\n",
    "4. I94 Immigration Data: comes from U.S. National Tourism and Trade Office containing details of immigrants coming into U.S. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, year, month, dayofmonth, round, regexp_replace\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from data_dictionary import i94_state_codes, i94_visa_codes, i94_country_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "    enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Demographics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "1. Convert columns that need to be aggregated into integer type\n",
    "2. Create race dataframe\n",
    "3. Create population dataframe\n",
    "4. Join dataframes together by state code\n",
    "5. Drop duplicates\n",
    "5. Rename columns to be able to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demographics = spark.read.csv(\"us-cities-demographics.csv\", sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert speicficed columns to integer type for summing capaibility\n",
    "df_demographics = df_demographics.withColumn(\"Count\", col(\"Count\").cast(IntegerType())) \\\n",
    ".withColumn(\"Total Population\", col(\"Total Population\").cast(IntegerType())) \\\n",
    ".withColumn(\"Male Population\", col(\"Male Population\").cast(IntegerType())) \\\n",
    ".withColumn(\"Female Population\", col(\"Female Population\").cast(IntegerType())) \\\n",
    ".withColumn(\"Foreign-born\", col(\"Foreign-born\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe containing population sums\n",
    "df_population_sum = df_demographics.groupBy(\"State Code\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------------+---------------------+-----------------+----------+\n",
      "|State Code|sum(Male Population)|sum(Female Population)|sum(Total Population)|sum(Foreign-born)|sum(Count)|\n",
      "+----------+--------------------+----------------------+---------------------+-----------------+----------+\n",
      "|        AZ|            11137275|              11360435|             22497710|          3411565|   5754881|\n",
      "|        SC|             1265291|               1321685|              2586976|           134019|    565751|\n",
      "|        LA|             3134990|               3367985|              6502975|           417095|   1391090|\n",
      "|        MN|             3478803|               3565362|              7044165|          1069888|   1546985|\n",
      "|        NJ|             3423033|               3507991|              6931024|          2327750|   1795916|\n",
      "+----------+--------------------+----------------------+---------------------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_population_sum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Race|\n",
      "+--------------------+\n",
      "|Black or African-...|\n",
      "|  Hispanic or Latino|\n",
      "|               White|\n",
      "|               Asian|\n",
      "|American Indian a...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct races\n",
    "df_demographics.select('Race').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe by pivoting  race column and tranform rows values to individual columns\n",
    "# Sum the races with the same state code\n",
    "df_race = df_demographics.groupBy(\"State Code\").pivot(\"Race\").sum(\"Count\").orderBy(\"State Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------------------+-------+-------------------------+------------------+--------+\n",
      "|State Code|American Indian and Alaska Native|  Asian|Black or African-American|Hispanic or Latino|   White|\n",
      "+----------+---------------------------------+-------+-------------------------+------------------+--------+\n",
      "|        AK|                            36339|  36825|                    23107|             27261|  212696|\n",
      "|        AL|                             8084|  28769|                   521068|             39313|  498920|\n",
      "|        AR|                             9381|  22062|                   149608|             77813|  384733|\n",
      "|        AZ|                           129708| 229183|                   296222|           1508157| 3591611|\n",
      "|        CA|                           401386|4543730|                  2047009|           9856464|14905129|\n",
      "+----------+---------------------------------+-------+-------------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_race.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join race and population dataframes to create final US demographics by state dataframe\n",
    "# Drop duplicates\n",
    "df_us_demographics = df_population_sum.join(df_race, \"State Code\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------------+---------------------+-----------------+----------+---------------------------------+------+-------------------------+------------------+-------+\n",
      "|State Code|sum(Male Population)|sum(Female Population)|sum(Total Population)|sum(Foreign-born)|sum(Count)|American Indian and Alaska Native| Asian|Black or African-American|Hispanic or Latino|  White|\n",
      "+----------+--------------------+----------------------+---------------------+-----------------+----------+---------------------------------+------+-------------------------+------------------+-------+\n",
      "|        AZ|            11137275|              11360435|             22497710|          3411565|   5754881|                           129708|229183|                   296222|           1508157|3591611|\n",
      "|        SC|             1265291|               1321685|              2586976|           134019|    565751|                             3705| 13355|                   175064|             29863| 343764|\n",
      "|        LA|             3134990|               3367985|              6502975|           417095|   1391090|                             8263| 38739|                   602377|             87133| 654578|\n",
      "|        MN|             3478803|               3565362|              7044165|          1069888|   1546985|                            25242|151544|                   216731|            103229|1050239|\n",
      "|        NJ|             3423033|               3507991|              6931024|          2327750|   1795916|                            11350|116844|                   452202|            600437| 615083|\n",
      "+----------+--------------------+----------------------+---------------------+-----------------+----------+---------------------------------+------+-------------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fix column names for correct format for parquet files\n",
    "df_us_demographics = df_us_demographics.select(col('State Code').alias('state_code'),\n",
    "                                               col('sum(Male Population)').alias('male_population'),\n",
    "                                               col('sum(Female Population)').alias('female_population'), \n",
    "                                               col('sum(Total Population)').alias('total_population'),\n",
    "                                               col('sum(Foreign-born)').alias('foreign_born'),\n",
    "                                               col('American Indian and Alaska Native').alias('american_indian_or_alaska_native'),\n",
    "                                               col('Asian').alias('asian'),\n",
    "                                               col('Black or African-American').alias('black_or_african_american'),\n",
    "                                               col('Hispanic or Latino').alias('hispanic_or_latino'),\n",
    "                                               col('White').alias('white')\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+----------------+------------+--------------------------------+------+-------------------------+------------------+-------+\n",
      "|state_code|male_population|female_population|total_population|foreign_born|american_indian_or_alaska_native| asian|black_or_african_american|hispanic_or_latino|  white|\n",
      "+----------+---------------+-----------------+----------------+------------+--------------------------------+------+-------------------------+------------------+-------+\n",
      "|        AZ|       11137275|         11360435|        22497710|     3411565|                          129708|229183|                   296222|           1508157|3591611|\n",
      "|        SC|        1265291|          1321685|         2586976|      134019|                            3705| 13355|                   175064|             29863| 343764|\n",
      "|        LA|        3134990|          3367985|         6502975|      417095|                            8263| 38739|                   602377|             87133| 654578|\n",
      "|        MN|        3478803|          3565362|         7044165|     1069888|                           25242|151544|                   216731|            103229|1050239|\n",
      "|        NJ|        3423033|          3507991|         6931024|     2327750|                           11350|116844|                   452202|            600437| 615083|\n",
      "+----------+---------------+-----------------+----------------+------------+--------------------------------+------+-------------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parquet demographics\n",
    "df_us_demographics.write.mode('overwrite').parquet(\"us_demographics_by_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Temperature Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "1. Filter by US only\n",
    "2. Break down datetime to year, month\n",
    "3. Create udf to convert state names to state codes\n",
    "4. Replace Georgia(State) to Georgia\n",
    "5. Convert Celcius to Fahrenheit\n",
    "6. Filter by year 2013, the most recent data\n",
    "7. Drop duplicates\n",
    "8. Rename columns to be able to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature = spark.read.csv(\"GlobalLandTemperaturesByState.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|State|Country|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|1855-05-01|            25.544|                        1.171| Acre| Brazil|\n",
      "|1855-06-01|            24.228|                        1.103| Acre| Brazil|\n",
      "|1855-07-01|            24.371|                        1.044| Acre| Brazil|\n",
      "|1855-08-01|            25.427|                        1.073| Acre| Brazil|\n",
      "|1855-09-01|            25.675|                        1.014| Acre| Brazil|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter the temperature by US only  \n",
    "df_us_temperature = df_temperature.filter(df_temperature[\"Country\"] == \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               State|\n",
      "+--------------------+\n",
      "|                Utah|\n",
      "|              Hawaii|\n",
      "|           Minnesota|\n",
      "|                Ohio|\n",
      "|            Arkansas|\n",
      "|              Oregon|\n",
      "|District Of Columbia|\n",
      "|     Georgia (State)|\n",
      "|               Texas|\n",
      "|        North Dakota|\n",
      "|        Pennsylvania|\n",
      "|         Connecticut|\n",
      "|            Nebraska|\n",
      "|             Vermont|\n",
      "|              Nevada|\n",
      "|          Washington|\n",
      "|            Illinois|\n",
      "|            Oklahoma|\n",
      "|            Delaware|\n",
      "|              Alaska|\n",
      "|          New Mexico|\n",
      "|       West Virginia|\n",
      "|            Missouri|\n",
      "|        Rhode Island|\n",
      "|             Montana|\n",
      "|            Michigan|\n",
      "|            Virginia|\n",
      "|      North Carolina|\n",
      "|             Wyoming|\n",
      "|              Kansas|\n",
      "|          New Jersey|\n",
      "|            Maryland|\n",
      "|             Alabama|\n",
      "|             Arizona|\n",
      "|                Iowa|\n",
      "|       Massachusetts|\n",
      "|            Kentucky|\n",
      "|           Louisiana|\n",
      "|         Mississippi|\n",
      "|       New Hampshire|\n",
      "|           Tennessee|\n",
      "|             Florida|\n",
      "|             Indiana|\n",
      "|               Idaho|\n",
      "|      South Carolina|\n",
      "|        South Dakota|\n",
      "|          California|\n",
      "|            New York|\n",
      "|           Wisconsin|\n",
      "|            Colorado|\n",
      "|               Maine|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_temperature.select('State').distinct().show(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reverse key value in i94addr_codes where key = full name and value = abbrev\n",
    "state_code_dict = dict((v, k) for k, v in i94_state_codes.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove (State) from Georgia \n",
    "df_us_temperature = df_us_temperature.withColumn('State', regexp_replace('State', 'Georgia \\(State\\)', 'Georgia'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    " # Define full name conversion to abbreviation udf \n",
    "state_abbrev_udf = udf(lambda x: state_code_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Break down datetime to year, month\n",
    "# Convert state names to state codes\n",
    "# Convert Celcius to Fahrenheit\n",
    "df_us_temperature = df_us_temperature.withColumn(\"year\", year(df_us_temperature[\"dt\"])) \\\n",
    ".withColumn(\"month\", month(df_us_temperature[\"dt\"])) \\\n",
    ".withColumn(\"state_code\", state_abbrev_udf(df_us_temperature[\"State\"])) \\\n",
    ".withColumn(\"average_temperature_fahrenheit\",round(df_us_temperature[\"AverageTemperature\"]*(9/5)+32,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+-------------+----+-----+----------+------------------------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|  State|      Country|year|month|state_code|average_temperature_fahrenheit|\n",
      "+----------+------------------+-----------------------------+-------+-------------+----+-----+----------+------------------------------+\n",
      "|1743-11-01|10.722000000000001|                        2.898|Alabama|United States|1743|   11|        AL|                          51.3|\n",
      "|1743-12-01|              null|                         null|Alabama|United States|1743|   12|        AL|                          null|\n",
      "|1744-01-01|              null|                         null|Alabama|United States|1744|    1|        AL|                          null|\n",
      "|1744-02-01|              null|                         null|Alabama|United States|1744|    2|        AL|                          null|\n",
      "|1744-03-01|              null|                         null|Alabama|United States|1744|    3|        AL|                          null|\n",
      "+----------+------------------+-----------------------------+-------+-------------+----+-----+----------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_temperature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter by year 2013, the most recent data\n",
    "df_us_temperature_2013 = df_us_temperature.filter(year(df_us_temperature[\"dt\"]) == 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+-------------+----+-----+----------+------------------------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|  State|      Country|year|month|state_code|average_temperature_fahrenheit|\n",
      "+----------+------------------+-----------------------------+-------+-------------+----+-----+----------+------------------------------+\n",
      "|2013-01-01|            10.284|                        0.241|Alabama|United States|2013|    1|        AL|                          50.5|\n",
      "|2013-02-01|             9.161|                        0.213|Alabama|United States|2013|    2|        AL|                          48.5|\n",
      "|2013-03-01|            10.226|                        0.158|Alabama|United States|2013|    3|        AL|                          50.4|\n",
      "|2013-04-01|            17.067|                        0.221|Alabama|United States|2013|    4|        AL|                          62.7|\n",
      "|2013-05-01|            20.619|                        0.229|Alabama|United States|2013|    5|        AL|                          69.1|\n",
      "+----------+------------------+-----------------------------+-------+-------------+----+-----+----------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_temperature_2013.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "# Pick columns and format for parquet\n",
    "df_us_temperature_2013 = df_us_temperature_2013.select(\"year\", \"month\", \"state_code\", \"average_temperature_fahrenheit\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+------------------------------+\n",
      "|year|month|state_code|average_temperature_fahrenheit|\n",
      "+----+-----+----------+------------------------------+\n",
      "|2013|    3|        FL|                          60.0|\n",
      "|2013|    6|        KY|                          74.0|\n",
      "|2013|    5|        ID|                          51.1|\n",
      "|2013|    6|        VA|                          73.0|\n",
      "|2013|    9|        WI|                          61.8|\n",
      "+----+-----+----------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_temperature_2013.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_temperature_2013.write.mode('overwrite').parquet(\"us_temperature_2013\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "df_airport_codes = pd.read_csv('airport-codes_csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Airport Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning steps\n",
    "1. Filter iso_region code by US only\n",
    "2. Convert airport codes to find the average elevation of each state\n",
    "3. Remove \"US-U-A\" from \"state_code\"\n",
    "3. Create udf to remove \"US-\" from iso_region to transform to \"state_code\"\n",
    "5. Convert elevation type to float\n",
    "6. Group by \"state_code\", finding averages of elevation\n",
    "7. Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_codes = spark.read.csv(\"airport-codes_csv.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Filter iso_region code by US only\n",
    "df_airport_codes = df_airport_codes.filter(df_airport_codes[\"iso_region\"].contains(\"US-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|iso_region|\n",
      "+----------+\n",
      "|     US-AK|\n",
      "|     US-AL|\n",
      "|     US-AR|\n",
      "|     US-AZ|\n",
      "|     US-CA|\n",
      "|     US-CO|\n",
      "|     US-CT|\n",
      "|     US-DC|\n",
      "|     US-DE|\n",
      "|     US-FL|\n",
      "|     US-GA|\n",
      "|     US-HI|\n",
      "|     US-IA|\n",
      "|     US-ID|\n",
      "|     US-IL|\n",
      "|     US-IN|\n",
      "|     US-KS|\n",
      "|     US-KY|\n",
      "|     US-LA|\n",
      "|     US-MA|\n",
      "|     US-MD|\n",
      "|     US-ME|\n",
      "|     US-MI|\n",
      "|     US-MN|\n",
      "|     US-MO|\n",
      "|     US-MS|\n",
      "|     US-MT|\n",
      "|     US-NC|\n",
      "|     US-ND|\n",
      "|     US-NE|\n",
      "|     US-NH|\n",
      "|     US-NJ|\n",
      "|     US-NM|\n",
      "|     US-NV|\n",
      "|     US-NY|\n",
      "|     US-OH|\n",
      "|     US-OK|\n",
      "|     US-OR|\n",
      "|     US-PA|\n",
      "|     US-RI|\n",
      "|     US-SC|\n",
      "|     US-SD|\n",
      "|     US-TN|\n",
      "|     US-TX|\n",
      "|    US-U-A|\n",
      "|     US-UT|\n",
      "|     US-VA|\n",
      "|     US-VT|\n",
      "|     US-WA|\n",
      "|     US-WI|\n",
      "|     US-WV|\n",
      "|     US-WY|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport_codes.select('iso_region').distinct().orderBy(\"iso_region\").show(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove iso region 'US-U-A'\n",
    "df_airport_codes = df_airport_codes.filter(df_airport_codes[\"iso_region\"] != \"US-U-A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create udf to remove \"US-\" from iso_region to transform to \"state_code\"\n",
    "get_state = udf(lambda x: x.replace(\"US-\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert elevation type to float\n",
    "df_airport_codes = df_airport_codes.withColumn(\"elevation_ft\", col(\"elevation_ft\").cast(FloatType()))\\\n",
    ".withColumn(\"iso_region\", get_state(col(\"iso_region\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Average elevation\n",
    "df_us_elevations = df_airport_codes.groupBy(\"iso_region\").avg(\"elevation_ft\").orderBy(\"iso_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Round values two decimal places\n",
    "df_us_elevations = df_us_elevations.select(col(\"iso_region\").alias(\"state_code\"),\\\n",
    "                                        round(col(\"avg(elevation_ft)\"),2).alias(\"avg_elevation_ft\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|state_code|avg_elevation_ft|\n",
      "+----------+----------------+\n",
      "|        AK|          447.09|\n",
      "|        AL|          437.11|\n",
      "|        AR|          517.48|\n",
      "|        AZ|         2710.69|\n",
      "|        CA|         1043.41|\n",
      "|        CO|         6200.13|\n",
      "|        CT|          329.78|\n",
      "|        DC|          176.38|\n",
      "|        DE|           83.36|\n",
      "|        FL|            70.2|\n",
      "|        GA|          675.69|\n",
      "|        HI|          668.85|\n",
      "|        IA|         1025.26|\n",
      "|        ID|         3750.29|\n",
      "|        IL|          677.32|\n",
      "|        IN|          759.16|\n",
      "|        KS|         1565.94|\n",
      "|        KY|          763.11|\n",
      "|        LA|           66.73|\n",
      "|        MA|           255.2|\n",
      "+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_elevations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_us_elevations.write.mode('overwrite').parquet(\"us_elevations_by_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Immigration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Data\n",
    "1. Drop Duplicates in original immigration dataframe\n",
    "2. Select columns for immigration dataframe\n",
    "3. Create dataframes state_codes, visa_codes, and residence_codes based on i94 sas labels \n",
    "4. Parquet all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration = spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_immigration = df_immigration.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select columns for dataframe \n",
    "df_immigration = df_immigration.select(col(\"i94yr\").alias(\"i94_year\"),\n",
    "                                       col(\"i94mon\").alias(\"i94_month\"),\n",
    "                                       col(\"i94res\").alias(\"i94_residence\"),\n",
    "                                       col(\"i94addr\").alias(\"i94_address\"),\n",
    "                                       col(\"i94visa\").alias(\"i94_visa\"),\n",
    "                                       \"gender\",\n",
    "                                       col(\"biryear\").alias(\"birth_year\")\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+-----------+--------+------+----------+\n",
      "|i94_year|i94_month|i94_residence|i94_address|i94_visa|gender|birth_year|\n",
      "+--------+---------+-------------+-----------+--------+------+----------+\n",
      "|  2016.0|      4.0|        103.0|       null|     2.0|     F|    1991.0|\n",
      "|  2016.0|      4.0|        104.0|         NY|     2.0|     F|    2000.0|\n",
      "|  2016.0|      4.0|        104.0|         FL|     2.0|     M|    1959.0|\n",
      "|  2016.0|      4.0|        104.0|         NY|     2.0|  null|    1965.0|\n",
      "|  2016.0|      4.0|        104.0|         FL|     2.0|  null|    2013.0|\n",
      "+--------+---------+-------------+-----------+--------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create state code abbreviations dataframe\n",
    "i94_state_codes_list = list(map(list, i94_state_codes.items()))\n",
    "df_i94_state_codes = spark.createDataFrame(i94_state_codes_list, [\"state_code\", \"state_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|state_code|state_name|\n",
      "+----------+----------+\n",
      "|        AL|   Alabama|\n",
      "|        AK|    Alaska|\n",
      "|        AZ|   Arizona|\n",
      "|        AR|  Arkansas|\n",
      "|        CA|California|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94_state_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create country code abbreviations dataframe\n",
    "i94_country_codes_list = list(map(list, i94_country_codes.items()))\n",
    "df_i94_country_codes = spark.createDataFrame(i94_country_codes_list, [\"country_code\", \"country_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|country_code|        country_name|\n",
      "+------------+--------------------+\n",
      "|         582|MEXICO Air Sea, a...|\n",
      "|         236|         AFGHANISTAN|\n",
      "|         101|             ALBANIA|\n",
      "|         316|             ALGERIA|\n",
      "|         102|             ANDORRA|\n",
      "+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94_country_codes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create visa code abbreviations dataframe\n",
    "i94_visa_codes_list = list(map(list, i94_visa_codes.items()))\n",
    "df_i94_visa_codes = spark.createDataFrame(i94_visa_codes_list, [\"visa_code\", \"visa_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|visa_code|visa_type|\n",
      "+---------+---------+\n",
      "|        1| Business|\n",
      "|        2| Pleasure|\n",
      "|        3|  Student|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i94_visa_codes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parquet all immigration, state, country, and visa dataframes \n",
    "df_immigration.write.mode('overwrite').partitionBy(\"i94_year\", \"i94_month\").parquet(\"immigration\")\n",
    "df_i94_state_codes.write.mode('overwrite').parquet(\"state_codes\")\n",
    "df_i94_country_codes.write.mode('overwrite').parquet(\"country_codes\")\n",
    "df_i94_visa_codes.write.mode('overwrite').parquet(\"visa_codes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The data model is a star schema because of its flexibility of querying whatever data we want and for easier analysis.\n",
    "\n",
    "\n",
    "1. Dimension Tables\n",
    " - demographics\n",
    "     - state_code\n",
    "     - male_population\n",
    "     - female_population\n",
    "     - total_population\n",
    "     - foreign_born\n",
    "     - american_indian_or_alaska_native\n",
    "     - asian, black_or_african_american\n",
    "     - hispanic_or_latino\n",
    "     - white\n",
    " - temperature\n",
    "     - year\n",
    "     - month\n",
    "     - state_code\n",
    "     - average_temperature_fahrenheit\n",
    " - elevation\n",
    "     - state_code\n",
    "     - avg_elevation_ft\n",
    " - state\n",
    "     - state_code\n",
    "     - state_name\n",
    " - country\n",
    "     - country_code\n",
    "     - country_name\n",
    " - visa\n",
    "     - visa_code\n",
    "     - visa_type\n",
    "\n",
    "\n",
    "2. Fact Table\n",
    " - immigration \n",
    "     - i94_year\n",
    "     - i94_month\n",
    "     - i94_residence (Immigrant's nationality)\n",
    "     - i94_address (The state that the immigrant is visiting)\n",
    "     - i94_visa\n",
    "     - gender\n",
    "     - birth_year\n",
    "     \n",
    "\n",
    "     \n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Create the spark dataframes for the dimension tables\n",
    "2. Create fact table that joins the dimension tables \n",
    "3. Parquet the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- Data Pipelines can be run with etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed. There are 49 rows in demographics. \n",
      "Passed. There are 51 rows in elevation. \n",
      "Passed. There are 459 rows in temperature. \n",
      "Passed. There are 3096313 rows in immigration. \n",
      "Passed. There are 52 rows in state. \n",
      "Passed. There are 289 rows in country. \n",
      "Passed. There are 3 rows in visa. \n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "#Count Test \n",
    "def row_count(df, name):\n",
    "    \"\"\"\n",
    "    Checks the row count of a dataframe.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    df -- spark dataframe\n",
    "    name -- string\n",
    "    \"\"\"\n",
    "    count= df.count()\n",
    "    if count > 0 :\n",
    "        print(\"Passed. There are {} rows in {}. \".format(count, name))\n",
    "    else:\n",
    "        print(\"Failed. There are 0 rows in {}.\".format(name))\n",
    "\n",
    "row_count(df_us_demographics, \"demographics\")\n",
    "row_count(df_us_elevations, \"elevation\")\n",
    "row_count(df_us_temperature_2013, \"temperature\")\n",
    "row_count(df_immigration, \"immigration\")\n",
    "row_count(df_i94_state_codes, \"state\")\n",
    "row_count(df_i94_country_codes, \"country\")\n",
    "row_count(df_i94_visa_codes, \"visa\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Integrity Test between dimension and fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+-----------+--------+------+----------+----------+\n",
      "|i94_year|i94_month|i94_residence|i94_address|i94_visa|gender|birth_year|state_name|\n",
      "+--------+---------+-------------+-----------+--------+------+----------+----------+\n",
      "|  2016.0|      4.0|        111.0|         AL|     2.0|     F|    1961.0|   Alabama|\n",
      "|  2016.0|      4.0|        687.0|         AL|     2.0|     M|    1964.0|   Alabama|\n",
      "|  2016.0|      4.0|        112.0|         AL|     1.0|     M|    1969.0|   Alabama|\n",
      "|  2016.0|      4.0|        245.0|         AL|     2.0|     F|    1969.0|   Alabama|\n",
      "|  2016.0|      4.0|        213.0|         AL|     1.0|     M|    1962.0|   Alabama|\n",
      "+--------+---------+-------------+-----------+--------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.join(df_i94_state_codes,df_immigration.i94_address == df_i94_state_codes.state_code).drop(df_i94_state_codes.state_code).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+-----------+--------+------+----------+---------------+-----------------+----------------+------------+--------------------------------+------+-------------------------+------------------+-------+\n",
      "|i94_year|i94_month|i94_residence|i94_address|i94_visa|gender|birth_year|male_population|female_population|total_population|foreign_born|american_indian_or_alaska_native| asian|black_or_african_american|hispanic_or_latino|  white|\n",
      "+--------+---------+-------------+-----------+--------+------+----------+---------------+-----------------+----------------+------------+--------------------------------+------+-------------------------+------------------+-------+\n",
      "|  2016.0|      4.0|        129.0|         AZ|     2.0|     M|    1977.0|       11137275|         11360435|        22497710|     3411565|                          129708|229183|                   296222|           1508157|3591611|\n",
      "|  2016.0|      4.0|        323.0|         AZ|     2.0|     M|    1960.0|       11137275|         11360435|        22497710|     3411565|                          129708|229183|                   296222|           1508157|3591611|\n",
      "|  2016.0|      4.0|        582.0|         AZ|     2.0|     F|    2008.0|       11137275|         11360435|        22497710|     3411565|                          129708|229183|                   296222|           1508157|3591611|\n",
      "|  2016.0|      4.0|        514.0|         AZ|     2.0|     F|    1977.0|       11137275|         11360435|        22497710|     3411565|                          129708|229183|                   296222|           1508157|3591611|\n",
      "|  2016.0|      4.0|        209.0|         AZ|     2.0|     F|    1980.0|       11137275|         11360435|        22497710|     3411565|                          129708|229183|                   296222|           1508157|3591611|\n",
      "+--------+---------+-------------+-----------+--------+------+----------+---------------+-----------------+----------------+------------+--------------------------------+------+-------------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.join(df_us_demographics,df_immigration.i94_address == df_us_demographics.state_code).drop(df_us_demographics.state_code).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    " - Data dictionaries will be found in data_dicitonary.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. I chose to use Apache Spark because of the ability to clean,transform, and  create outputs that have large data  \n",
    "2. The data should be updated monthly because I94 immigration data is updated monthly. We can perform more analysis and find more trends if data is updated monthly.\n",
    "3. Approaching different scenarios:\n",
    "- If data was increased by 100, then I would create an EMR cluster on AWS for handling even larger data. I can scale and and more nodes if needded.\n",
    "- If the data needs to be populated daily, then I would implement Apache airflow to schedule piplines to run at 7am every day.\n",
    "- If data needs to be accessed by 100+ people, then I would implement Amazon Redshift so that they database will be able to handle big data queries as well as queries made by multiple users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
